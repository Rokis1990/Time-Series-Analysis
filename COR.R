# set working directory below and store the data files in a subfolder called Datasetwd("~/Manish_Kumar_Assignments/2017_09_08")
# load the libraries library(e1071) library(data.table)library(xlsx)library(mlbench)library(DMwR)library(party) # Random Forest for variable selectionlibrary(earth) # implements variable importance based on Generalized cross validation (GCV), number of subset models the variable occurs (nsubsets) and residual sum of squares (RSS).library(zoo)library(tseries)library(forecast)library(caTools)library(corrplot)library(PerformanceAnalytics) # for chart.Correlationlibrary(Boruta)library(rpart)library(rpart.plot)library(ROCR)library(randomForest)library(caret)library(DAAG)library(car) # for VIFlibrary(lmtest) # for Breusch-Pagan Testlibrary(betareg)
# load the outcome variablesrealEstateLoans <- fread("~/Manish_Kumar_Assignments/2017_09_08/Data/real_estate_loans.csv")ccLoans <- fread("~/Manish_Kumar_Assignments/2017_09_08/Data/credit_card_loans.csv")ciLoans <- fread("~/Manish_Kumar_Assignments/2017_09_08/Data/commercial_and_industrial_loans.csv")agriLoans <- fread("~/Manish_Kumar_Assignments/2017_09_08/Data/agricultural_production_loans.csv")
# load the historical macroeconomic variables domesticVars <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 1,rowIndex = 4:72, colIndex = 2:19, header = T)internationalVars <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 1,rowIndex = 77:145, colIndex = 2:15, header = T)
names(domesticVars) <- c('Quarter','Year','RealGDPGrowth',                         'NominalGDPGrowth','Real_DIG','Nominal_DIG','UnempRate',                         'CPI_InflRate','TrRt3mo','TrYld5y','TrYld10y',                         'BBB_CorpYld','MortgageRate','PrimeRate','DJIA',                         'HousePriceIndex','CommercialREIndex','MarketVIX')names(internationalVars) <- c('Quarter','Year','EuroAreaRGDPGrowth',                              'EuroAreaInflation','fx_USD_Euro',                              'DevelopingAsiaRGDPGrowth',                              'DevelopingAsiaInflation','fx_dASIA_USD',                              'JapanRGDPGrowth','JapanInflation',                              'fx_Yen_USD', 'UKRGDPGrowth','UKInflation',                              'fx_USD_GBP')
# consolidate the macro variablesmacros <- cbind(domesticVars,                 internationalVars[,3:ncol(internationalVars)])
# consolidate the outcome and macro variables # Note for dat variable declaration below: # outcome data row 62 corresponds to 2000 Q1# ... and row 129 corresponds to 2016 Q4
dat <- cbind(data.frame(Quarter = macros$Quarter, Year = macros$Year,                        AgriLoans = agriLoans$CORLAGT100S[62:129],                        CCLoans = ccLoans$CORCCT100S[62:129],                        CILoans = ciLoans$CORBLT100S[62:129],                        RELoans = realEstateLoans$CORSRET100S[62:129]),             macros[,3:ncol(macros)])
# store the 'dat' data frame as model_train.csv in desired directorywrite.csv(dat, file = "C:/Users/AnirudhJ/Documents/Manish_Kumar_Assignments/2017_09_08/Data/model_train.csv")
# First, we transform the data a bit to suit our time series analysis needstimeSTAMP <- as.yearqtr(paste(dat$Year, dat$Quarter, sep = " "))dat$Year <- timeSTAMP dat$Quarter <- NULL # Remove the 'Quarter' variablenames(dat)[1] <- 'YearQuarter' # change the variable namedat$YearQuarter <- as.Date(dat$YearQuarter)# dat$QVar <- factor(month(dat$YearQuarter))dat$YearQuarter <- NULL
for(i in 1:4){  dat[,i] <- dat[,i]/100}
# Remove certain variable objects from memoryrm(agriLoans);rm(ccLoans);rm(ciLoans);rm(realEstateLoans)rm(domesticVars); rm(internationalVars)rm(macros)
### Create 4 Datasets Agri <- dat[which(dat[,1] > 0),c(1,5:ncol(dat))] CC <- dat[,c(2,5:ncol(dat))]CI <- dat[,c(3,5:ncol(dat))]RE <- dat[,c(4,5:ncol(dat))]
### Modeling Agriculture Loans Charge Off Rates ---------------------# dat <- Agri# response <- dat[,1]# variables <- dat[,-1]
### Check the distributions of the outcome variables ------------------------plot(density(Agri$AgriLoans))plot(density(CC$CCLoans))plot(density(CI$CILoans))plot(density(RE$RELoans))
# Does it make sense to do a log-transform?plot(density(log(Agri$AgriLoans)))plot(density(log(CC$CCLoans)))plot(density(log(CI$CILoans)))plot(density(log(RE$RELoans)))

### Automating the Modeling ------------------------------------------models_rf <- list()models_beta <- list()
set.seed(1)
for(i in 1:4){  if(i == 1){    dat <- Agri  } else if(i == 2){    dat <- CC   } else if(i == 3){    dat <- CI  } else{    dat <- RE  }  response <- dat[,1]; variables <- dat[,-1]    # Visualizing Correlations  corrplot(cor(dat))    # get summary statistics for all columns  sapply(dat, summary)  
  # Box Cox Transformations  sk <- skewness(response, na.rm=T)   if(sk > 1){    boxcoxMod <- BoxCoxTrans(response) # calculates lambda value     dat[,1] <- predict(boxcoxMod, dat[,1])  }    # Box-Cox Transforming the predictor variables  boxcoxTransformedVars <- character(0)  # variables that underwent box-cox transformation collects here.  for(colname in colnames(dat)[-1]){    x <- dat[,colname]    if(abs(skewness(x, na.rm=T)) > 1){  # check for high skewness.       boxcoxMod <- BoxCoxTrans(x)  # calculates lambda value and store in a model      boxcoxTransformedVars <<- c(boxcoxTransformedVars, colname)      dat[, colname] <- predict(boxcoxMod, x)  # calculate transformed variable          }  }    # Checking for Significance  significantPreds_cont <- character()  # initialise output. Significant preds will accrue here    for (predName in names(variables)) {    pred <- dat[, predName]    mod <- lm(response ~ pred)  # build linear model with only current predictor    p_value <- summary(mod)$coefficients[, 4][2]  # capture p-Value    if (p_value < 0.1 & p_value > 0) {  # check for significance      significantPreds_cont <- c (significantPreds_cont, predName)  # if selected, bind the predictor name if main output    }  }    inputData_cont_signif <- dat[, names(dat) %in% significantPreds_cont]  # filter selected predictors    # Put together the input data with only significant variables  inputData_signif <- cbind(response, inputData_cont_signif)    # Variable Selection    # Decide if a variable is important or not using Boruta  boruta_output <- Boruta(response ~ ., data=na.omit(inputData_signif), doTrace=2)  # perform Boruta search  boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables  # Selecting best variables (variable selection using cforest)  cf1 <- cforest(response ~ . , data= na.omit(inputData_signif[, names(inputData_signif) %in% c(boruta_signif, "response")]), control=cforest_unbiased(mtry=2,ntree=50)) # fit the random forest  impVars <- sort(varimp(cf1), decreasing=T) # get variable importance  # Pick top 7. Adjust this as per your needs. More variables will take more computational time.  impVars <- if(length(impVars) > 7){    impVars[1:7]  } else {    impVars  }    filtered_variables <- names(impVars)    # Create all combinations of selected variables that will go into models as predictors  # create all combinations of predictors  max_model_size <- length(filtered_variables)  combos_matrix <- matrix(ncol=max_model_size)  # initialise final output  for (n in 1:length(filtered_variables)){    combs_mat <- t(combn(filtered_variables, n))  # all combinations of variable    nrow_out <- nrow(combs_mat)    ncol_out <- length(filtered_variables)    nrow_na <- nrow_out    ncol_na <- ncol_out-ncol(combs_mat)    na_mat <- matrix(rep(NA, nrow_na*ncol_na), nrow=nrow_na, ncol=ncol_na)    out <- cbind(combs_mat, na_mat)    combos_matrix <- rbind(combos_matrix, out)  }    combos_matrix <- combos_matrix[-1, ]  # remove the first row that has all NA    # Create Training and Test Sets    ## Random Sampling -----------------------------------------------  set.seed(100)  spl <- sample.split(response, SplitRatio = 0.7)  inputData_signif_training <- subset(inputData_signif, spl == T)  inputData_signif_test <- subset(inputData_signif, spl == F)    ## Non-random Sampling (done mostly for TS data) -----------------  splRatio <- 0.8; spl <- ceiling(splRatio * nrow(inputData_signif))  inputData_signif_training <- inputData_signif[1:spl,]  inputData_signif_test <- inputData_signif[(spl+1):nrow(inputData_signif),]
    # Building all the linear models and diagnostics  ### BUILD THE LINEAR MODELS    Final_Output <- data.frame()  # initialise the final output dataframe    for (rownum in 1:nrow(combos_matrix)){    ## Build model for current formula from combos_matrix-    preds <- na.omit(combos_matrix[rownum, ])  # get the predictor names    form <- paste ("response ~ ", paste (preds, collapse=" + "), sep="")  # model formula    # currMod <- lm(as.formula(form), data=inputData_signif_training)  # build the linear model    # currSumm <- summary(currMod)  # model summary    set.seed(111) # set random seed for model reusability    rfMod <- randomForest(as.formula(form), data = inputData_signif_training) # build a Random Forest Model    betaMod <- betareg(as.formula(form), data = inputData_signif_training)    betaSumm <- summary(betaMod)        ## Diagnostic parameters-    beta_phi <- betaSumm$coefficients$precision[1]    beta_p_value <- betaSumm$coefficients$precision[4]    beta_pseudo_r_sq <- betaSumm$pseudo.r.squared    aic <- AIC(betaMod)  # AIC    bic <- BIC(betaMod)  # BIC        ## Calculate Random Forest accuracy on Development sample -    preds_train <- predict(rfMod, inputData_signif_training)    actual_fitted_rf <- data.frame(actuals = inputData_signif_training$response, fitted = preds_train)    min_vals <- apply(actual_fitted_rf, 1, min)    max_vals <- apply(actual_fitted_rf, 1, max)    training_min_max_rf <- min_vals / max_vals    training_min_max_accuracy_rf <- mean(training_min_max_rf)        ## Calculate Random Forest accuracy on Validation sample -    preds_test <- predict(rfMod, inputData_signif_test)    actual_predicted_rf <- data.frame(actuals = inputData_signif_test$response, predicted = preds_test)    min_vals <- apply(actual_predicted_rf, 1, min)    max_vals <- apply(actual_predicted_rf, 1, max)    testing_min_max_rf <- min_vals / max_vals    testing_min_max_accuracy_rf <- mean(testing_min_max_rf)     ## Calculate Beta Regression accuracy on Development sample -    training_mape <- mean((abs(betaMod$residuals))/abs(na.omit(inputData_signif_training$response))) * 100  # MAPE    beta_preds_train <- predict(betaMod, inputData_signif_training)    actual_fitted_beta <- data.frame(actuals = inputData_signif_training$response, fitted = beta_preds_train)    min_vals <- apply(actual_fitted_beta, 1, min)    max_vals <- apply(actual_fitted_beta, 1, max)    training_min_max_beta <- min_vals / max_vals    training_min_max_accuracy_beta <- mean(training_min_max_beta)        ## Calculate Beta Regression accuracy on Validation sample -    beta_preds_test <- predict(betaMod, inputData_signif_test)    predicted_mape <- mean((abs(inputData_signif_test$response - beta_preds_test)/abs(inputData_signif_test$response))) * 100  # calculate predicted mape      actual_predicted_beta <- data.frame(actuals = inputData_signif_test$response, predicted = beta_preds_test)    min_vals <- apply(actual_predicted_beta, 1, min)    max_vals <- apply(actual_predicted_beta, 1, max)    testing_min_max_beta <- min_vals / max_vals    testing_min_max_accuracy_beta <- mean(testing_min_max_beta)           ## Collect all stats for Final Output-    currOutput <- data.frame(formula=form,                             Phi = beta_phi, Pseudo.Rsq = beta_pseudo_r_sq,                             AIC=aic, BIC=bic, Model.pValue= beta_p_value,                              training.mape=training_mape,                              predicted.mape=predicted_mape,                              Random_Forest_Training_Accuracy = training_min_max_accuracy_rf,                              Random_Forest_Testing_Accuracy = testing_min_max_accuracy_rf,                              BetaReg_Training_Accuracy = training_min_max_accuracy_beta,                              BetaReg_Testing_Accuracy = testing_min_max_accuracy_beta)        # Final Output    Final_Output <- rbind(Final_Output, currOutput)        # ## Print output so they get accumulated in 'All_Models_In_Combos.txt'    # names(actual_fitted) <- c("actuals", "predicted")    # actuals_predicted_all <- rbind(actual_fitted, actual_predicteds)    # print (currSumm)    # print (actuals_predicteds_predictors <- cbind(actuals_predicted_all, na.omit(inputData_signif)[, preds]))  }  if(i == 1){    write.csv(Final_Output, "Agri_Regression_Output.csv", row.names=F)  # Export  } else if(i == 2){    write.csv(Final_Output, "CC_Regression_Output.csv", row.names=F)  # Export  } else if(i==3){    write.csv(Final_Output, "CI_Regression_Output.csv", row.names=F)  # Export  } else {    write.csv(Final_Output, "RE_Regression_Output.csv", row.names=F)  # Export  }  models_rf[[i]] <- randomForest(as.formula(as.character(Final_Output$formula[which.max(Final_Output$Random_Forest_Testing_Accuracy)])),                                  data = inputData_signif_training)  models_beta[[i]] <- lm(as.formula(as.character(Final_Output$formula[which.max(Final_Output$BetaReg_Testing_Accuracy)])),                            data = inputData_signif_training)  }
Agri_Output <- fread('Agri_Regression_Output.csv')CC_Output <- fread('CC_Regression_Output.csv')CI_Output <- fread('CI_Regression_Output.csv')RE_Output <- fread('RE_Regression_Output.csv')
# Prediction Accuracies on Testing Set (Random Forest Model)max(Agri_Output$Random_Forest_Testing_Accuracy) # 0.7261732max(CC_Output$Random_Forest_Testing_Accuracy) # 0.9034877max(CI_Output$Random_Forest_Testing_Accuracy) # 0.7675963max(RE_Output$Random_Forest_Testing_Accuracy) # 0.7068117
# Prediction Accuracies on Testing Set (Beta Regression Model)max(Agri_Output$BetaReg_Testing_Accuracy) # 0.7463428max(CC_Output$BetaReg_Testing_Accuracy) # 0.8783635max(CI_Output$BetaReg_Testing_Accuracy) # 0.7271575max(RE_Output$BetaReg_Testing_Accuracy) # 0.6837077
### Read 9 Quarters' Data -------------------------------------------
# Read the macro variables for base, adverse and severely adverse cases
# Base Case:domesticVars9Q_base <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 2,rowIndex = 4:17, colIndex = 4:19, header = T)internationalVars9Q_base <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 2,rowIndex = 22:35, colIndex = 4:15, header = T)
names(domesticVars9Q_base) <- c('RealGDPGrowth',                         'NominalGDPGrowth','Real_DIG','Nominal_DIG','UnempRate',                         'CPI_InflRate','TrRt3mo','TrYld5y','TrYld10y',                         'BBB_CorpYld','MortgageRate','PrimeRate','DJIA',                         'HousePriceIndex','CommercialREIndex','MarketVIX')
names(internationalVars9Q_base) <- c('EuroAreaRGDPGrowth',                              'EuroAreaInflation','fx_USD_Euro',                              'DevelopingAsiaRGDPGrowth',                              'DevelopingAsiaInflation','fx_dASIA_USD',                              'JapanRGDPGrowth','JapanInflation',                              'fx_Yen_USD', 'UKRGDPGrowth','UKInflation',                              'fx_USD_GBP')
# consolidate the macro variables for base casemacros9Q_base <- cbind(domesticVars9Q_base,                 internationalVars9Q_base)
# Adverse Case:domesticVars9Q_adverse <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 3,rowIndex = 4:17, colIndex = 4:19, header = T)internationalVars9Q_adverse <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 3,rowIndex = 22:35, colIndex = 4:15, header = T)
names(domesticVars9Q_adverse) <- names(domesticVars9Q_base)names(internationalVars9Q_adverse) <- names(internationalVars9Q_base)
# consolidate the macro variables for adverse casemacros9Q_adverse <- cbind(domesticVars9Q_adverse,                           internationalVars9Q_adverse)
# Severely Adverse Case:domesticVars9Q_SevAdverse <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 4,rowIndex = 4:17, colIndex = 4:19, header = T)internationalVars9Q_SevAdverse <- read.xlsx(file = '~/Manish_Kumar_Assignments/2017_09_08/Data/macro_data.xlsx',sheetIndex = 4,rowIndex = 22:35, colIndex = 4:15, header = T)
names(domesticVars9Q_SevAdverse) <- names(domesticVars9Q_base)names(internationalVars9Q_SevAdverse) <- names(internationalVars9Q_base)
# consolidate the macro variables for severely adverse casemacros9Q_SevAdverse <- cbind(domesticVars9Q_SevAdverse,                           internationalVars9Q_SevAdverse)
### PREDICTIONS UNDER BASE, ADVERSE AND SEVERELY ADVERSE CASES -----
base_predictions <- list()adverse_predictions <- list()sevAdverse_predictions <- list()
